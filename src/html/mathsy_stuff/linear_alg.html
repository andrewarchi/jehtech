<!DOCTYPE HTML>
<html>
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <!-- HTML 4 -->
	<meta charset="UTF-8">                                              <!-- HTML 5 -->
	<title>Linear Algebra</title>
	<!-- META_INSERT -->
	<!-- CSS_INSERT -->
	<!-- JAVASCRIPT_INSERT -->
</head>

<body>
<div id="header">
	-- This is JEHTech --
</div>

<div id="sidebar">
	<h1 class="title">Links...</h1>
	<div id="includedContent"></div>
</div>

<div id="content">
<h1 class="title">Linear Algebra Notes</h1>
<div style="padding-right:10px;">
<p>
	Notes on some linear algebra topics. Sometimes I find things are just stated
	as being obvious, for example, <q>the dot product of two orthogonal vectors
	is zero</q>. Well... why is this? The result was a pretty simple but
	nonetheless it took a little while to think it through properly.
	Hence, these notes... they're my musings on the &quot;why&quot;, even if the
	&quot;why&quot; might be obvious to the rest of the world!
</p>
<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
<h2>Page Contents</h2>
<div id="page_contents">
</div>

<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
<h2>Vector Stuff</h2>
<div>
<h3>Linear Combinations</h3>
<p>
    A linear combination of vectors is one that only uses the addition and scalar multiplication
    of those variables. So, given two vectors, $\vec a$ and $\vec b$ the following are linear
    combinations:
    $$
        0\vec a + 0\vec b \\
        5\vec a - 2.5\vec b \\
        -123\vec a + \pi\vec b
    $$
    This can be generalise to say that for any set of vectors
    $\{\vec{v_1}, \vec{v_2}, \ldots, \vec{v_n}\} \text{ for } \vec{v_1}, \vec{v_2}, \ldots, \vec{v_n} \in \mathbb{R}^m$
    that a linear combination of those vectors is $c_1\vec{v_1} + c_2\vec{v_2} + \cdots + c_n\vec{v_n} \text{ for } c_i \in \mathbb{R}$.
</p>

<h3>Spans</h3>
<p>
    <img style="float:right;" src="##IMG_DIR##/span.png" alt="graph showing span of two vectors in 2D space"/>
    The <em>set of all linear combinations</em> of a set of vectors $\{\vec{v_1}, \vec{v_2}, \ldots, \vec{v_n}\}$ is called the <em>span</em> of those vectors:

    $$
        \mathrm{span}(\vec{v_1}, \vec{v_2}, \ldots, \vec{v_n}) = \{c_1\vec{v_1} + c_2\vec{v_2} + \cdots + c_n\vec{v_n} \;|\; c_i \in \mathbb{R}\}
    $$

    In the graph to the right there are two vectors $\vec a$ and $\vec b$. The dotted blue lines show
    all the possible scalings of the vectors. The vectors span those lines. The green lines show
    two particular scalings of $\vec a$ added to all the possible scalings of $\vec b$. One can
    use this to imagine that if we added all the possible scalings of $\vec a$ to all the possible
    scalings of $\vec b$ we would reach every coordinate in $\mathbb{R}^2$. Thus, we can say that
    the set of vectors $\{\vec a, \vec b\}$ spans $\mathbb{R}^2$, or $\mathrm{span}(\vec a, \vec b) = \mathbb{R}^2$.
</p>
<p>
    From this we may be tempted to say that any set of two, 2d, vectors spans $\mathbb{R}^2$ but we
    would be wrong. Take for example $\vec a$ and $\vec b = -\alpha\vec a$. These two variables are
    <a href="https://en.wikipedia.org/wiki/Collinearity" target="_blank">co-linear</a>. Because
    they span the same line, no combination of them can ever move off that line. Therefore they
    do <em>not</em> span $\mathbb{R}^2$.
</p>
<p>
    Lets write the above out a little more thoroughly...

    $$
        \vec a =
        \begin{bmatrix}
          a_1 \\
          a_2
        \end{bmatrix}

        , \vec b =
        \begin{bmatrix}
          -\alpha a_1 \\
          -\alpha a_2
        \end{bmatrix}

    $$

    The span of these vectors is, by our above definition:

    $$
    \mathrm{span}(\vec a, \vec b) =
    \left\{
        c_1
        \begin{bmatrix}
          a_1 \\
          a_2
        \end{bmatrix}

        + c_2
        \begin{bmatrix}
          -\alpha a_1 \\
          -\alpha a_2
        \end{bmatrix}

        \mid c_i \in \mathbb{R}
    \right\}
    $$

    Which we can re-write as:

    $$
    \begin{align}
    \mathrm{span}(\vec a, \vec b) &=
    \left\{
        c_1
        \begin{bmatrix}
          a_1 \\
          a_2
        \end{bmatrix}

        -\alpha c_2
        \begin{bmatrix}
          a_1 \\
          a_2
        \end{bmatrix}

        \mid c_i \in \mathbb{R}
    \right\} \\

    &=
    \left\{
        (c_1 -\alpha c_2)
        \begin{bmatrix}
          a_1 \\
          a_2
        \end{bmatrix}
        \mid c_i \in \mathbb{R}
    \right\} \\
    \end{align}
    $$

    ... which we can clearly see is just the set of all scaled vectors of $\vec a$. Plainly,
    any co-linear set of 2d vectors will not span $\mathbb{R}^2$.
</p>

<h3 style="clear:both;">Linear Independence</h3>
<p>
    We saw above that the set of vectors $\{\vec a, -\alpha\vec a\}$ are co-linear and thus did
    not span $\mathbb{R}^2$. The reason for this is that they are linealy <em>dependent</em>, which
    means that <em>one or more vectors in the set are just linear combinations of other vectors
    in the set</em>.
</p>
<p>
    For example, the set of vectors $\{[1, 0], [0, 1], [2, 3]\}$ spans $\mathbb{R}^2$, but $[2, 3]$ is
    redundant because we can remove it from the set of vectors and still have a set that spans $\mathbb{R}^2$.
    In other words, it adds no new information or directionality to our set.
</p>
<p>
    A set of vectors will be said to be linearly <em>dependent</em> when:

    $$
        c_1v_1 + \cdots + c_nv_n = \vec 0 \mid \exists \{c_i | c_i \ne 0 \}
    $$

    Sure, this is a definition, but why does this mean that the set is linearly <em>dependent</em>?
</p>
<div>
    <img style="float:left; margin-right:10px; margin-bottom:10px;" src="##IMG_DIR##/linearly-independent-1.png" alt="Picture of linearly indpendent vectors"/>
    <p>
        Think of vectors in a $\mathbb{R}^2$. For any two vectors that are not co-linear, there is
        no combination of those two vectors that can get you back to the origin unless they are both
        scaled by zero. The image to the left is trying to demonstrate that. If we scale $\vec a$, no matter
        how small we make that scaling, we cannot find a scaling of $\vec b$ that when added to $\vec a$
        will get us back to the origin. Thus we cannot find a linear combination $c_av_a + c_bv_b = \vec 0$ where
        $c_a$ and/or $c_b$ does not equal zero.
    </p>
    <p>
        This shows that neither vector is a linear combination of the other, because if it where, we could
        use a non-zero scaling of one, added to the other, to get us back to the origin.
    </p>
    <p>
        We can also show that two vectors are linearly independent when their dot product is zero. This
        is covered in a latter section.
    </p>
</div>

<h3 style="clear:both">Linear Subspace</h3>
<p>
    If $V$ is a set of vectors in $\mathbb{R}^n$, then $V$ is a <em>subspace</em> of $\mathbb{R}^n$
    if and only if:
</p>
<ol>
    <li><b>$V$ contains the zero vector</b>: $\vec 0 \in V$</li>
    <li><b>$V$ is closed under scalar multiplication</b>: $\vec x \in V, \alpha \in \mathbb{R} \implies \alpha\vec x \in V$</li>
    <li><b>$V$ is closed under addition</b>: $\vec a \in V, \vec b \in V \implies \vec a + \vec b \in V$</li>
</ol>
</div> <!-- END H2 -->

<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
<h2 style="clear:both;">Matricies And Simultaneous Linear Equations</h2>
<div>
<p>
    The set of linear equations:

    $$ a_{11}x_1 + a_{12}x_2 + ... + a_{1n}x_n = b_1\\
       a_{21}x_1 + a_{22}x_2 + ... + a_{2n}x_n = b_2\\
       ... \\
       a_{m1}x_1 + a_{m2}x_2 + ... + a_{mn}x_n = b_m
    $$

    Can be represented in matrix form:

    $$
     \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn} \\
     \end{bmatrix}

     \begin{bmatrix}
        x_1 \\
        x_2 \\
        \vdots \\
        x_n \\
     \end{bmatrix}

     =

     \begin{bmatrix}
        b_1 \\
        b_2 \\
        \vdots \\
        b_m \\
    \end{bmatrix}
    $$

    Any vector $\vec x$, where $A\vec x = \vec b$, is a solution to the set of simultaneous
    linear equations. How can we solve this from the matrix. We create an <b>augmented or patitioned matrix</b>:

    $$
        \left[
            \begin{array}{cccc|c}
                1a_{11} & a_{12} & \cdots & a_{1n} & b_1\\
                a_{21} & a_{22} & \cdots & a_{2n} & b_2 \\
                \vdots & \vdots & \ddots & \vdots & \vdots \\
                a_{m1} & a_{m2} & \cdots & a_{mn} & b_m\\
            \end{array}
        \right]
    $$

    Next this matrix must be converted to <b>row echelon form</b>. This is where each row has
    the same or less zeros to the right than the row below. For example, the following matrix is
    in row echelon form:

    $$
     \begin{bmatrix}
        1 & 2 & 3 & 4 \\
        0 & 1 & 2 & 3 \\
        0 & 0 & 0 & 1 \\
        0 & 0 & 0 & 2 \\
     \end{bmatrix}
    $$

    But, this next one is not:
     \begin{bmatrix}
        1 & 2 & 3 & 4 \\
        0 & 0 & 0 & 1 \\
        0 & 0 & 0 & 2 \\
        0 & 1 & 2 & 3 \\
     \end{bmatrix}


    We go from a matrix to the equivalent in row-echelon form using <b>elementary row operations</b>:
</p>
<ol>
    <li>Row interchange: $R_i \leftrightarrow R_j$</li>
    <li>Row scaling: $R_i \rightarrow kR_i$</li>
    <li>Row addition: $R_i \rightarrow R_i + kR_j$</li>
</ol>
<p>
    We can do this in python using the <code>sympy</code> package and the <code>rref()</code> function:
</p>
<pre class="prettyprint linenums">from sympy import Matrix, init_printing

init_printing(use_unicode=True)

system = Matrix(( (3, 6, -2, 11), (1, 2, 1, 32), (1, -1, 1, 1) ))
system
system.rref()[0]</pre>
<p>
    Which outputs:
</p>
<pre>⎡1  0  0  -17/3⎤
⎢              ⎥
⎢0  1  0  31/3 ⎥
⎢              ⎥
⎣0  0  1   17  ⎦</pre>
<p>
    To then apply this to our augmented matrix we could do something like the following. Lets
    say we have a set of 3 simultaneous equations with 3 unknowns:

    $$
        3x + 6y - 2z = 11 \\
        x + 2y + z = 32 \\
        x - y + z = 1
    $$

    We'd make an augmented matrix like so:

    $$
        \left[
            \begin{array}{ccc|c}
                3 & 6 & -2 & 11\\
                1 & 2 & 1 & 32\\
                1 & -1 & 1 & 1
            \end{array}
        \right]
    $$

    To solve this using Python we do:
</p>
<pre class="prettyprint linenums">from sympy import Matrix, solve_linear_system, init_printing
from sympy.abc import x, y, z

init_printing(use_unicode=True)

system = Matrix(( (3, 6, -2, 11), (1, 2, 1, 32), (1, -1, 1, 1) ))
system

solve_linear_system(system, x, y, z)</pre>
<p>
    Which will give the following output:
</p>
<pre>⎡3  6   -2  11⎤
⎢             ⎥
⎢1  2   1   32⎥
⎢             ⎥
⎣1  -1  1   1 ⎦
{x: -17/3, y: 31/3, z: 17}</pre>
</div>

<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
<h2>Orthogonal Vectors and Linear Independence</h2>
<div>
	<div style="float:right; border: 1px solid gray; padding: 10px; margin:10px;">
		<img src="##IMG_DIR##/3d_vectors_at_right_angles.png" alt="Image showing 3 perpendicular vectors" />
		<br>
		<i>Figure 1</i>
	</div>
	<p>
		<b>Orthogonal vectors</b> are vectors that are perpendicular to each other.
		In a standard 2D or 3D graph, this means that
		they are at right angles to each other and we can visualise them
		as seen in Figure 1: $\vec x$ is at 90 degrees to both $\vec y$ and $\vec z$.
		$\vec y$ is at 90 degrees to $\vec x$ and $\vec z$, and $\vec z$ is at
		90 degrees to $\vec y$ and $\vec z$.
	</p>
	<p>
		In any set of any vectors, $[\vec{a_1}, ..., \vec{a_n}]$, the vectors are
		said to be linearly <b>linearly independent</b> if every vector in the set
		is orthogonal to every other vector in the set.
	</p>
	<p>
		So, how do we tell if one vector is orthogonal to another? The answer is
		the <b>dot product</b> which is defined as follows.

		$$x \cdot y = \sum_1^n {x_i y_i}$$
	</p>
	<p>
		We know when two <b>vectors are orthogonal when their dot product is
		zero: $x \cdot y = 0 \implies$ x and y are orthogonal</b>.
	</p>
	<p>But why is this the case? Lets imagine any two aribtrary vectors, each
		on the circumference of a unit circle (so we know that they have the
		same length and are therefore a proper rotation of a vector around the
		centre of the circle). This is shown in figure 2. From the figure, we
		know the following:
	</p>

	<div style="float:right; border:1px solid gray; margin:10px; padding:10px;">
		<img src="##IMG_DIR##/rotate_vector.png" alt="Picture showing a vector being rotated"/>
		<br/>
		Figure 2
	</div>

	<p>
		$$
		\begin{align}
		x_1 &= r \cos{\theta_1}& y_1 &= r \sin{\theta_1} \\
		x_2 &= r \cos({\theta_1 + \theta_n}) & y_2 &= r \sin({\theta_1 + \theta_n})
		\end{align}
		$$

		The vector $x_2$ is the vector $x_1$ rotated by $\theta_n$ degrees.
		We can use the following
		<a href="math_revision.html#sum_and_product_trig_formulas" target="_blank"> trig identities</a>:

		$$
		\begin{align}
		\sin(a \pm b)& = \sin(a)\cos(b) \pm \cos(a)\sin(b) \\
		\cos(a \pm b)& = \cos(a)\cos(b) \mp \sin(a)\sin(b)
		\end{align}
		$$

		Substitute these into the formauls above, we get the following.

		$$
		\begin{align}
		x_2 &= r\cos\theta_1\cos\theta_n - r\sin\theta_1\sin\theta_n \\
		y_2 &= r\sin\theta_1\cos\theta_n + r\cos\theta_1\sin\theta_n \\
		\end{align}
		$$

		Which means that...

		$$
		\begin{align}
		x_2 &= x_1\cos\theta_n - y_1\sin\theta_n \\
		y_2 &= y_1\cos\theta_n + x_1\sin\theta_n
		\end{align}
		$$

		For a 90 degree rotation $\theta_n = 90^\circ$ we know that $\cos\theta_n = 0$
		and $\sin\theta_n = 1$. Substiuting these values into the above equations
		we can clearly see that...

		$$
		\begin{align}
		x_2 &= -y_1 \\
		y_2 &= x_1
		\end{align}
		$$

		Therefore, any vector in 2D space that is $[a, b]$ will become
		$[-b, a]$ and therefore the dot product becomes $-ab + ab = 0$. And
		voilà, we know know <b>why</b> the dot product of two orthogonal 2D vectors is
		zero. I'm happy to take it on faith that this extrapolates into n dimensions :)
	</p>
    <p>
        Another way of looking at this is to consider the law of cosines: $C^2 = A^2 + B^2 - 2AB\cos(c)$:
    </p>
    <p>
        <img src="##IMG_DIR##/law_of_cosines_1.png" alt="Law of cosines"/>
    </p>
        Here $A$ and $V$ represent the magnitude of the vectors $\vec x$, $\|\vec x\|$, and $\vec y$, $\|\vec y\|$. $C$ represents
        the magnitude of the vector $\|\vec x - \vec y\|$. Thus, we have this:
    </p>
    <p>
        <img src="##IMG_DIR##/law_of_cosines_2.png" alt="Law of cosines"/>
    </p>
    <p>
        I.e., $\|\vec x - \vec y\|^2 = \|\vec x\|^2 + \|\vec y\|^2 - 2\|\vec x\|\|\vec y\|\cos(\theta_c)$.
    </p>
    <p>
        When the angle between the vectors is 90 degrees, $\cos(90) = 0$ and so that part of the
        equation dissapears to give us $\|\vec x - \vec y\|^2 = \|\vec x\|^2 + \|\vec y\|^2$.
        Now...

        $$
        \|\vec x - \vec y\|^2 = \|\vec x\|^2 + \|\vec y\|^2 \\
        \therefore \|\vec x\|^2 + \|\vec y\|^2 - \|\vec x - \vec y\|^2 = 0
        $$

        Recall that $\|\vec v\| = \sqrt{v_1^2 + v_2^2 + ... + v_n^2}$ and that therefore $\|\vec v\|^2 = v_1^2 + v_2^2 + ... + v_n^2$ .
    </p>
    <p>
        Based, on this we can say that...

        $$
            \begin{align}
            \|\vec x - \vec y\|^2 &= (x_1 - y_1)^2 + (x_2 - y_2)^2 + ... + (x_n - y_n)^2 \\
                                  &= x_1^2 - 2x_1y_1 + y_1^2 + x_2^2 - 2x_2y_2 + y_2^2 + ... + x_n^2 - 2x_ny_n + y_n^2
            \end{align}
        $$

        Now, all of the $x_i^2$ and $y_i^2$ terms above will be cancelled out by the $\|\vec x\|^2$ and $\|\vec y\|^2$ terms,
        leaving us with...

        $$
        \|\vec x\|^2 + \|\vec y\|^2 - \|\vec x - \vec y\|^2 = 2x_1y_1 + 2x_2y_2 + ... + 2x_ny_n = 0
        $$

        Which is just twice the dot product when the vectors are at 90 degrees - or just the dot product because we can divide through by 2.
    </p>
</div>

<h2>Orthogonal Matrices</h2>
<div>
	<p>
		So, onto orthogonal matrices. A <b>matrix is orthogonal if $AA^T = A^TA = I$</b>.
		If we take a general matrix and multiply it by it's transpose we get...
	</p>

	$$
		AA^T =
		\begin{pmatrix}
		a & c \\
		b & d \\
		\end{pmatrix}
		\begin{pmatrix}
		a & b \\
		c & d \\
		\end{pmatrix}
		=
		\begin{pmatrix}
		aa + cc & ab + cd \\
		ab + cd & bb + dd \\
		\end{pmatrix}
		= \begin{pmatrix}
		1 & 0 \\
		0 & 1 \\
		\end{pmatrix}
	$$

	<p>
		The pattern $ab + cd$ looks pretty familiar right?! It looks a lot like
		$x_1x_2 + y_1y_2$, our formula for the dot product of two vectors. So,
		when we say $a=x_1$, $b=x_2$, $c=y_1$ and $d=y_2$, we will get the following:
		a matrix of two <em>row vectors</em> $\vec v_1 = [x_1, y_1]$, $\vec v_2 = [x_2, y_2]$.

		$$
			A =
			\begin{pmatrix}
			\vec v_1 \\
			\vec v_2 \\
			\end{pmatrix} =
			\begin{pmatrix}
			x_1 & y_1 \\
			x_2 & y_2 \\
			\end{pmatrix}
		$$

		$$
		\therefore AA^T =
		\begin{pmatrix}
		x_1 & y_1 \\
		x_2 & y_2 \\
		\end{pmatrix}
		\begin{pmatrix}
		x_1 & x_2 \\
		y_1 & y_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
		x_1^2 + y_1^2 & x_1x_2 + y_1y_2 \\
		x_1x_2 + y_1y_2 & x_2^2 + y_2^2 \\
		\end{pmatrix}
		$$

		If our vectors $\vec v_1$ and $\vec v_2$ are orthogonal,
		then the component $x_1x_2 + y_1y_2$ must be zero. This would give us the first
		part of the identify matrix pattern we're looking for.
	</p>
	<p>
		The other part of the identity matrix would imply that we would have
		to have $x_1^2 + y_1^2 = 1$ and $x_2^2 + y_2^2 = 1$... which are just the
		formulas for the square of the length of a vector.
		Therefore, if our two vectors are normal (i.e, have a length of 1), we
		have our identity matrix.
	</p>
	<p>
		Does the same hold true for $A^TA$? It doesn't if we use our original
		matrix A!...

	$$
		A^TA =
		\begin{pmatrix}
		x_1 & x_2 \\
		y_1 & y_2 \\
		\end{pmatrix}
		\begin{pmatrix}
		x_1 & y_1 \\
		x_2 & y_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
		x_1^2 + x_2^2 & x_1y_1 + x_2y_2 \\
		x_1y_1 + x_2y_2 & y_1^2 + y_2^2 \\
		\end{pmatrix}
	$$

		Oops, we can see that we didn't get the identity matrix!! But, perhaps we
		can see why. If $A$ was a matrix of row vectors then $A^T$ is a matrix
		of column vectors. So for $AA^T$ we were multiplying a matrix of row vectors
		with a matrix of column vectors, which would, in part, give us the dot
		products as we saw. So if we want to do $A^TA$ it would follow that for
		this to work $A$ now was to be a matrix of column vectors because
		we get back to our original ($A^T$ would become a mtrix of row vectors):

	$$
		A^TA =
		\begin{pmatrix}
		x_1 & y_1 \\
		x_2 & y_2 \\
		\end{pmatrix}
		\begin{pmatrix}
		x_1 & x_2 \\
		y_1 & y_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
		x_1^2 + y_1^2 & x_1x_2 + y_1y_2 \\
		x_1x_2 + y_1y_2 & x_2^2 + y_2^2 \\
		\end{pmatrix}
	$$

	</p>

	<p>
		So we can say that if we have <b>matrix who's rows are orthogonal vectors
			and who's columns are also orthogonal vectors, then we have
		an orthogonal matrix</b>!
	</p>
	<p>
		Okay, thats great n' all, but <b>why should we care? Why Are orthogonal
		matricies useful?</b>. It turns our that orthogonal matricies
		<a href="https://www.khanacademy.org/math/linear-algebra/alternate_bases/orthonormal_basis/v/lin-alg-orthogonal-matrices-preserve-angles-and-lengths" target="_blank">preserve
		angles and lengths</a> of vectors. This can be useful in graphics to rotate
		vectors but keep the shape they construct, or in numerical analysis
		because <a href="http://www.math.utah.edu/~pa/6610/20130916.pdf" target="_blank">they do not amplify errors</a>.
</div>

<h2>Determinants</h2>
<div>
	<p>
		This is one of the better YouTube tutorials I've found that explains the <i>concepts</i> and not just the
		mechanics behind determinants, byt <a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw" target="blank">3BlueBrown</a> (awesome!).
	</p>
	<iframe width="560" height="315" src="https://www.youtube.com/embed/Ip3X9LOh2dk?rel=0" frameborder="0" allowfullscreen></iframe>
</div>

<h2>Eigenvectors and Eigenvalues</h2>
<div>
	<p>
		<a href="http://math.stackexchange.com/questions/23312/what-is-the-importance-of-eigenvalues-eigenvectors" target="_blank">Stackoverflow thread: What is the importance of eigenvalues and eigenvectors</a>.
	</p>
	<p>
		<a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues">another</a>
	</p>
	<blockquote>
		<p>Eigenvectors and values exist in pairs: every eigenvector has a corresponding eigenvalue. An eigenvector is a direction, ... An eigenvalue is a number, telling you how much variance there is in the data in that direction ...</p>
		<footer><a href="https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/"
		           target="_blank">Principal Component Analysis 4 Dummies: Eigenvectors, Eigenvalues and Dimension Reduction</a>a, G. Dallas
		</footer>
	</blockquote>
	<p></p>
</div>

<h2>PCA</h2>
<div>
	<p>
		Really liked these YouTube tutorials by <a href="https://www.youtube.com/channel/UC-A3RB9zleSwKGYBdBcV8lw" target="_blank">Data4Bio</a>:
	</p>
	<ol>
		<li><a href="https://www.youtube.com/watch?v=sIyMAzAHw6I" target="_blank">Dimensionality Reduction: High Dimensional Data, Part 1</a></li>
 		<li><a href="https://www.youtube.com/watch?v=0IjQ0SyQEOI" target="_blank">Dimensionality Reduction: High Dimensional Data, Part 2</a></li>
 		<li><a href="https://www.youtube.com/watch?v=OXQtmRtgEF4" target="_blank">Dimensionality Reduction: High Dimensional Data, Part 3</a></li>
		<li><a href="https://www.youtube.com/watch?v=ZqXnPcyIAL8" target="_blank">Dimensionality Reduction: Principal Components Analysis, Part 1</a></li>
 		<li><a href="https://www.youtube.com/watch?v=NUn6WeFM5cM" target="_blank">Dimensionality Reduction: Principal Components Analysis, Part 2</a></li>
 		<li><a href="https://www.youtube.com/watch?v=7WqWoEKUdQY" target="_blank">Dimensionality Reduction: Principal Components Analysis, Part 3</a></li>
	</ol>

</div>

</div> <!-- END: H1 padding div -->
</div> <!-- END: H1 div -->

</body>
</html>


